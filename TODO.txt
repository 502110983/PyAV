The Big Plan
============

- Restore audio resampling
- Uniform time representation
- Restore video encoding
- Restore audio encoding
- Examples for encoding from numpy arrays
- Lots of documentation


Encoding
========

- Should video frame dimensions be a property of Stream, Codec, or VideoFormat?
  It feels most accurate for there to be an immutable (but writable) VideoFormat
  on a Context. If we want it on Stream, it should proxy attribute access to
  the underlying Codec.
- Why do we need to much separation between Streams and Codecs? Since they are
  created at the same time, and the user will not create their own Codec (will
  they?), then they could be merged together.
    - Streams have a Codec, timing information, and metadata.
    - Codec has all the dimensions, qualities, encoding details, etc..


Unsorted
========

- Rename Context to Container.
- Create Format descriptor
- Merge Codec into Stream
- Create Codec Descriptor

- Uniform time representation: fraction or av.utils.Rational
  
  1) av.utils.Rational:

    - Needs: __abs__, __add__, __div__, __eq__, __floordiv__, __le__, __lt__,
      __mod__, __mul__, __neg__, __pos__, __pow__, __radd__, __rdiv__, __rfloordiv__,
      __rmod__, __rmul__, __rpow__, __rtruediv__, __truediv__, __trunc__,
      denominator, numerator

    - Likely doable as a Mixin.
    - Is the extra little bit of speed worth the trouble?

  2) fraction.Fraction:

    - Slower:
      $ python -m timeit -s 'from fractions import Fraction; a = Fraction(1, 2)' 'b = a * 1234567890'
      100000 loops, best of 3: 9.95 usec per loop

  - I think that the Python and C objects will not have synchonized time stamps
    since the user will need to set the time on the frame without a stream. So
    either (1) the user needs to manage both the stream time_base AND the frame
    times, or (2) the Frame.time needs to be compatible with the stream time_base

  - For now, for making it move quickly, use the same system, and call it `pts.
    Then, in the future, we can introduce `time` attributes and make it cleaner.
  - Automatically set reasonable default `time_base`s determined by the frame
    and sample rates, such that a pts of 1 is either a sample or a frame.

runtime_library_dirs


- Replace AudioFrame.resample with AudioResampler

    - make examples.decode use it for audio

    - see: detailed description on http://www.ffmpeg.org/doxygen/2.0/group__lswr.html
                               and https://libav.org/doxygen/master/group__lavr.html

    - FFmpeg's swr does not have an internal fifo. This is why we must always
      assert that we are getting everything we request.
    - Separate the AudioResampler from the AudioFifo; the user must manage
      them both if they want to both resample and rechunk the stream

    - AudioResampler(format, layout)
        .convert(frame) -> a new frame with (nearly) the same logical data
            - The first time this is called it initializes the internal src_format
              and src_layout. After that, it asserts that they are equal, or
              raises a ValueError since the incoming packet does not match
        .flush() or .convert(None) -> last frames

    - AudioFifo(format, layout, nb_samples=0)
        .write(frame)
        .read(samples=0) -> AudioFrame of specified samples or ALL the samples

- Context use a StringIO or similar to write into/from?
    - Create wrapper around AVIOContext for arbitrary Python object with read/write
      methods.

- libavdevice? Pull from video input?

- Various AudioLayout/AudioFormat/VideoFormat attributes should be writable.
    - is_mutable flags on various objects (including formats, layouts, contexts,
      streams, etc.) could guard the __set__ methods of properties.

- Should methods be: to_bytes, as_bytes, tobytes, asbytes??

- Plane.array_format could be a format string for array.array
- Plane.update_from_array(array.array)
    - If it were HD RGB then it would be 1920 * 1080 * 3 long.
- Plane.update_from_ndarray(numpy.ndarray)
    - If it were HD RGB then it would have shape (1080, 1920, 3)
- Plane.update(input_)
    And then it tries if it is a buffer, memoryview, bytes, etc..

- Stream.encode(...) -> list of packets, automatically checking buffer)
- Context.mux(...) -> take a single packet, or an iterator of them:
        context.mux(stream.encode(frame))

- Context.add_stream(codec_name, frame_rate) -> Context.streams.append(Stream(codec_name, frame_rate))?

- SwsContext -> av.video.rescaler.Rescaler
- SwrContext -> av.autio.resample.Resampler

- `make test-assets` -> into tests/assets/
- TestCase.rms_diff(one, two) -> Root-mean-square diff
- try to wrap API of testsrc filters
- Vagrant for two environments for ffmpeg and libav
    libav needs to have LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib set for
    both building and testing. Also try `runtime_library_dirs`.

- Humanize attribute names?
    pts -> time
    pkt_time -> Packet.time


- Split Context into input/output varieties.

- FFmpeg tutorial: http://dranger.com/ffmpeg/
	- also has function reference: http://dranger.com/ffmpeg/functions.html
	- updated tutorial code: https://github.com/chelyaev/ffmpeg-tutorial

- Even out more of the differences:
    - See README of https://github.com/chelyaev/ffmpeg-tutorial

- Should Packet.decode yield Frames, or return a list of Frames?

- VideoStream.setup_conversion(size, format, etc.)

- Move decoding into Packet from Stream?

- Replicate av_frame_get_best_effort_timestamp
    http://ffmpeg.org/pipermail/ffmpeg-devel/2011-February/104327.html
    http://pastebin.com/Aq8eDZw3/
    http://web.archiveorange.com/archive/v/yR2T4bybpYnYCUXmzAI5

